{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1ZD31r2uf-Ux"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4TETlBEcuG3",
        "outputId": "1a689920-6658-4cb4-90d7-61efe101c753"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla V100-SXM2-16GB\n",
            "Sat Apr 20 16:07:52 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla V100-SXM2-16GB           Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0              24W / 300W |      2MiB / 16384MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "    !nvidia-smi\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_YuRDWzcv8g",
        "outputId": "9d45ed7c-8167-438a-dfcb-0a3bf1438877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting farasapy==0.0.14\n",
            "  Downloading farasapy-0.0.14-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from farasapy==0.0.14) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from farasapy==0.0.14) (4.66.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy==0.0.14) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy==0.0.14) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy==0.0.14) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy==0.0.14) (2024.2.2)\n",
            "Installing collected packages: farasapy\n",
            "Successfully installed farasapy-0.0.14\n",
            "Collecting pyarabic==0.6.14\n",
            "  Downloading PyArabic-0.6.14-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.4/126.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from pyarabic==0.6.14) (1.16.0)\n",
            "Installing collected packages: pyarabic\n",
            "Successfully installed pyarabic-0.6.14\n",
            "Cloning into 'arabert'...\n",
            "remote: Enumerating objects: 600, done.\u001b[K\n",
            "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 600 (delta 38), reused 45 (delta 30), pack-reused 535\u001b[K\n",
            "Receiving objects: 100% (600/600), 9.14 MiB | 15.73 MiB/s, done.\n",
            "Resolving deltas: 100% (339/339), done.\n",
            "Collecting emoji==1.6.1\n",
            "  Downloading emoji-1.6.1.tar.gz (170 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.0/170.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.1-py3-none-any.whl size=169297 sha256=883b16206fd1f8bddbcac97b8db5199e33152be5e031fa2179c791768c46e7d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/c9/af/02caa5725634f27f4e2e43852f67fc9069d014038b236a827e\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.6.1\n",
            "Collecting sentencepiece==0.1.96\n",
            "  Downloading sentencepiece-0.1.96-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "  Attempting uninstall: sentencepiece\n",
            "    Found existing installation: sentencepiece 0.1.99\n",
            "    Uninstalling sentencepiece-0.1.99:\n",
            "      Successfully uninstalled sentencepiece-0.1.99\n",
            "Successfully installed sentencepiece-0.1.96\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.4.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=50bb5cf0b401199a56f718040874732a733785dc6ab1ce9c8ad248f178bdd175\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install farasapy==0.0.14\n",
        "!pip install pyarabic==0.6.14\n",
        "!git clone https://github.com/aub-mind/arabert\n",
        "!pip install emoji==1.6.1\n",
        "!pip install sentencepiece==0.1.96\n",
        "!pip install seqeval\n",
        "#!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "djzOxAz4x7sV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d661e887-3029-444f-93c9-7ddf4fb2474e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'WNER_24_sharedtask'...\n",
            "remote: Enumerating objects: 73, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 73 (delta 25), reused 68 (delta 20), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (73/73), 53.12 KiB | 5.90 MiB/s, done.\n",
            "Resolving deltas: 100% (25/25), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/AhmedAbdel-Aal/WNER_24_sharedtask.git\n",
        "!cp -r WNER_24_sharedtask/code/*.py .\n",
        "!rm -rf WNER_24_sharedtask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Z89S1TxOcv_M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import copy\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from arabert.preprocess import ArabertPreprocessor\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import (AutoConfig, AutoModel, AutoModelForSequenceClassification,\n",
        "                          AutoTokenizer, BertTokenizer, Trainer,\n",
        "                          TrainingArguments)\n",
        "from transformers.data.processors.utils import InputFeatures\n",
        "\n",
        "from arabert.preprocess import ArabertPreprocessor\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoConfig, AutoModelForTokenClassification, AutoTokenizer\n",
        "from transformers import Trainer , TrainingArguments\n",
        "from transformers.trainer_utils import EvaluationStrategy\n",
        "from transformers.data.processors.utils import InputFeatures\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.utils import resample\n",
        "import logging\n",
        "import torch\n",
        "from utils import *\n",
        "from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Uij3wvOKcwBt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48a604d5-5138-441e-ae5b-62cf62c797fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data loaded from path /content/drive/MyDrive/anlp24/WojoodNER_fine_Data/split70.json\n",
            "data loaded from path /content/drive/MyDrive/anlp24/WojoodNER_fine_Data/split10.json\n"
          ]
        }
      ],
      "source": [
        "train_data = load_json('/content/drive/MyDrive/anlp24/WojoodNER_fine_Data/split70.json')\n",
        "dev_data = load_json('/content/drive/MyDrive/anlp24/WojoodNER_fine_Data/split10.json')\n",
        "\n",
        "def normalize_data(items):\n",
        "    normalized_data = []\n",
        "    text_lists = []\n",
        "    label_list = []\n",
        "    for item in items:\n",
        "        sentence_id = item['global_sentence_id']\n",
        "        sentence = []\n",
        "        labels = []\n",
        "        for token_info in item['tokens']:\n",
        "            word = token_info['token']\n",
        "            tv = token_info['tags'][0]\n",
        "            label = tv['value']\n",
        "            sentence.append(word)\n",
        "            labels.append(label)\n",
        "        #sentence = ' '.join(sentence)\n",
        "        #labels = ' '.join(labels)\n",
        "        text_lists.append(sentence)\n",
        "        label_list.append(labels)\n",
        "    return text_lists, label_list\n",
        "\n",
        "\n",
        "# Normalize the data\n",
        "text_train, labels_train = normalize_data(train_data)\n",
        "text_dev, labels_dev = normalize_data(dev_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MNhtv4vJdqbd"
      },
      "outputs": [],
      "source": [
        "# prompt: load the saved model\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/anlp24/nerflat-arabertv02-e3/output_dir\")\n",
        "model_loaded = AutoModelForTokenClassification.from_pretrained(\"/content/drive/MyDrive/anlp24/nerflat-arabertv02-e3/output_dir\").to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "z2ISKV9kyg7u"
      },
      "outputs": [],
      "source": [
        "label_map = model_loaded.config.label2id\n",
        "inv_label_map = model_loaded.config.id2label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9i6YXb3ZcwEV"
      },
      "outputs": [],
      "source": [
        "from data import NERDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jyuCGIq9cwG5"
      },
      "outputs": [],
      "source": [
        "model_name = 'aubmindlab/bert-base-arabertv02'\n",
        "task_name = 'tokenclassification'\n",
        "tokenizer_name = \"/content/drive/MyDrive/anlp24/nerflat-arabertv02-e3/output_dir\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_pIJRVA0cwLV"
      },
      "outputs": [],
      "source": [
        "train_dataset = NERDataset(\n",
        "    texts=text_train,\n",
        "    tags=labels_train,\n",
        "    label_map=label_map,\n",
        "    model_name=model_name,\n",
        "    tokenizer_name=tokenizer_name,\n",
        "    max_length=512,\n",
        "    )\n",
        "\n",
        "dev_dataset = NERDataset(\n",
        "    texts=text_dev,\n",
        "    tags=labels_dev,\n",
        "    label_map=label_map,\n",
        "    model_name=model_name,\n",
        "    tokenizer_name=tokenizer_name,\n",
        "    max_length=512,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed=42):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  torch.backends.cudnn.deterministic=True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(25)"
      ],
      "metadata": {
        "id": "E9dqRNx89MmH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "CA7mUXDg4dMd"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate_fn function to handle batching of InputFeatures.\n",
        "\n",
        "    Args:\n",
        "    - batch: A list of InputFeatures instances.\n",
        "\n",
        "    Returns:\n",
        "    - A dictionary with keys 'input_ids', 'attention_mask', 'token_type_ids', 'labels',\n",
        "      where the values are batched tensors.\n",
        "    \"\"\"\n",
        "    # Initialize containers for the various features\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    token_type_ids = []\n",
        "    labels = []\n",
        "    texts = []\n",
        "\n",
        "\n",
        "    for item in batch:\n",
        "        #print(item)\n",
        "        input_ids.append(torch.tensor(item['input_ids']))  # Convert to tensor\n",
        "        attention_masks.append(torch.tensor(item['attention_mask']))  # Convert to tensor\n",
        "        token_type_ids.append(torch.tensor(item['token_type_ids']))  # Convert to tensor\n",
        "        labels.append(torch.tensor(item['labels']))\n",
        "        texts.append(item['text'])\n",
        "\n",
        "    # Convert lists to tensors and stack them\n",
        "    input_ids = torch.stack(input_ids)\n",
        "    attention_masks = torch.stack(attention_masks)\n",
        "    token_type_ids = torch.stack(token_type_ids)\n",
        "    labels = torch.stack(labels)\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_masks,\n",
        "        'token_type_ids': token_type_ids,\n",
        "        'label': labels,\n",
        "        'texts':texts\n",
        "    }\n",
        "\n",
        "# Prepare DataLoader'\n",
        "#batch_size = 16  # Or any batch size that fits your memory\n",
        "#train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False, num_workers=5)\n",
        "#test_loader = DataLoader(dev_dataset, batch_size=batch_size,collate_fn=collate_fn, shuffle=False, num_workers=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3cUuz8yz5KTE"
      },
      "outputs": [],
      "source": [
        "batch_size = 16  # Or any batch size that fits your memory\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False, num_workers=5)\n",
        "test_loader = DataLoader(dev_dataset, batch_size=batch_size,collate_fn=collate_fn, shuffle=False, num_workers=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnQhVE5f5LPT"
      },
      "source": [
        "# Infere - Model Only"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def align_predictions_2(predictions, label_ids):\n",
        "    predictions = predictions.detach().cpu().numpy()\n",
        "    label_ids = label_ids.detach().cpu().numpy()\n",
        "    preds = np.argmax(predictions, axis=2)\n",
        "\n",
        "    batch_size, seq_len = preds.shape\n",
        "\n",
        "    ignored_index = torch.nn.CrossEntropyLoss().ignore_index\n",
        "\n",
        "    valid_mask = label_ids != ignored_index\n",
        "\n",
        "    # Use boolean indexing to filter out the embeddings and logits\n",
        "    filtered_preds = preds[valid_mask]\n",
        "    filtered_labels = label_ids[valid_mask]\n",
        "\n",
        "    # Map labels using inv_label_map, vectorized operation\n",
        "    out_label_list = [inv_label_map[label.item()] for label in filtered_labels]\n",
        "    preds_list = [inv_label_map[label.item()] for label in filtered_preds]\n",
        "\n",
        "    assert len(preds_list) == len(out_label_list)\n",
        "    return preds_list, out_label_list"
      ],
      "metadata": {
        "id": "au3TUjE4_2FX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7TbORiK5T_X",
        "outputId": "c7337d9e-e0be-416f-8362-9765be181567"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/207 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "100%|██████████| 207/207 [00:33<00:00,  6.11it/s]\n"
          ]
        }
      ],
      "source": [
        "text_tokens = []\n",
        "predicted_tag_1 = []\n",
        "true_tags = []\n",
        "model_loaded.eval()\n",
        "for batch in tqdm(test_loader):\n",
        "    batch = {k: v.to(device) if 'texts' not in k else v for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        token_type_ids = batch['token_type_ids']\n",
        "        labels = batch['label']\n",
        "        outputs = model_loaded(input_ids, attention_mask, token_type_ids)\n",
        "        preds, aligned_true_tags = align_predictions_2(outputs.logits, labels)\n",
        "\n",
        "        text_tokens.append(batch['texts'])\n",
        "        predicted_tag_1.append(preds)\n",
        "        true_tags.append(aligned_true_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YOiPEhCKGAbB"
      },
      "outputs": [],
      "source": [
        "assert len(predicted_tag_1) == len(true_tags)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
        "from seqeval.scheme import IOB2\n",
        "r = {\n",
        "  'acc:       ': accuracy_score(true_tags, predicted_tag_1),\n",
        "  \"micro_f1\": f1_score(true_tags, predicted_tag_1, average=\"micro\", scheme=IOB2),\n",
        "  \"macro_f1\": f1_score(true_tags, predicted_tag_1, average=\"macro\", scheme=IOB2),\n",
        "  \"weights_f1\": f1_score(true_tags, predicted_tag_1, average=\"weighted\", scheme=IOB2),\n",
        "  \"precision\": precision_score(true_tags, predicted_tag_1, scheme=IOB2),\n",
        "  \"recall\": recall_score(true_tags, predicted_tag_1, scheme=IOB2)}\n",
        "r"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhooNqigAFSp",
        "outputId": "4535e2f5-2ac8-476f-96b7-3eba4f3e3767"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'acc:       ': 0.9729612316888804,\n",
              " 'micro_f1': 0.9076582803428257,\n",
              " 'macro_f1': 0.7357219022042203,\n",
              " 'weights_f1': 0.908298002364155,\n",
              " 'precision': 0.8967495219885278,\n",
              " 'recall': 0.9188357122865939}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsWRl0pZ6jVf"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_index(embeddings, label_ids):\n",
        "    batch_size, seq_len, _ = embeddings.shape\n",
        "    label_ids = label_ids.detach().cpu().numpy()\n",
        "    embeddings = embeddings.detach().cpu().numpy()\n",
        "\n",
        "    ignored_index = torch.nn.CrossEntropyLoss().ignore_index\n",
        "\n",
        "\n",
        "    valid_mask = label_ids != ignored_index\n",
        "\n",
        "    # Use boolean indexing to filter out the embeddings and logits\n",
        "    filtered_embeddings = embeddings[valid_mask]\n",
        "    filtered_labels = label_ids[valid_mask]\n",
        "\n",
        "    # Map labels using inv_label_map, vectorized operation\n",
        "    out_label_list = [inv_label_map[label.item()] for label in filtered_labels]\n",
        "\n",
        "    assert len(filtered_embeddings) == len(filtered_labels)\n",
        "    return torch.tensor(filtered_embeddings), out_label_list"
      ],
      "metadata": {
        "id": "rmWzawdptmbs"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_datastore(model, loader, device):\n",
        "    train_embeddings = []\n",
        "    train_labels = []\n",
        "\n",
        "    model.eval()\n",
        "    for batch in tqdm(loader):\n",
        "        batch = {k: v.to(device) if 'texts' not in k else v for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            input_ids = batch['input_ids']\n",
        "            attention_mask = batch['attention_mask']\n",
        "            token_type_ids = batch['token_type_ids']\n",
        "            labels = batch['label']\n",
        "            outputs = model(input_ids, attention_mask, token_type_ids,output_hidden_states=True)\n",
        "            embeddings = outputs.hidden_states[-1]\n",
        "\n",
        "            in_embeddings, in_labels = create_index(embeddings, labels)\n",
        "            train_embeddings.extend(in_embeddings)\n",
        "            train_labels.extend(in_labels)\n",
        "    return np.vstack(train_embeddings), np.vstack(train_labels)"
      ],
      "metadata": {
        "id": "rQnDgI_mtBc4"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datastore_keys, datastore_values = create_datastore(model_loaded, train_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hi_JZ23TCPnt",
        "outputId": "645184de-3492-4ae9-bf31-61dd587ef87c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1446 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "100%|██████████| 1446/1446 [03:50<00:00,  6.26it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datastore_keys.shape, datastore_values.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_cLUpRqCPrD",
        "outputId": "8fcd2e97-83e6-4d31-e7c8-ed8f949edb21"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((390900, 768), (390900, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save datastore_keys and datastore_values as .npy files\n",
        "\n",
        "import numpy as np\n",
        "np.save('/content/drive/MyDrive/anlp24/nerflat-arabertv02-e3/output_dir2/datastore_keys.npy', datastore_keys)\n",
        "np.save('/content/drive/MyDrive/anlp24/nerflat-arabertv02-e3/output_dir2/datastore_values.npy', datastore_values)\n"
      ],
      "metadata": {
        "id": "sGyEAsDeifJH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: load datastore_keys and values from .npy files\n",
        "\n",
        "import numpy as np\n",
        "datastore_keys = np.load('/content/drive/MyDrive/anlp24/nerflat-arabertv02-e3/output_dir/datastore_keys.npy')\n",
        "datastore_values = np.load('/content/drive/MyDrive/anlp24/nerflat-arabertv02-e3/output_dir/datastore_values.npy')\n"
      ],
      "metadata": {
        "id": "uK4KqJ2afjKo"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datastore_values_mapped = torch.tensor(datastore_values)\n",
        "datastore_values_mapped.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnlfJKU3kJiX",
        "outputId": "09549cd2-f572-4668-cc93-fc7008f3b58e"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([390900])"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datastore_values = datastore_values.reshape(-1)\n",
        "datastore_values_mapped = [label_map[x] for x in datastore_values]\n",
        "datastore_values_mapped = torch.tensor(datastore_values_mapped)\n",
        "datastore_values_mapped.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "V30m8BT0zww0",
        "outputId": "331ba81f-2923-4c68-9293-c4bb661a75f0"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-128-b9ade36cc86f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdatastore_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatastore_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdatastore_values_mapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatastore_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdatastore_values_mapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatastore_values_mapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdatastore_values_mapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-128-b9ade36cc86f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdatastore_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatastore_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdatastore_values_mapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatastore_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdatastore_values_mapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatastore_values_mapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdatastore_values_mapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys = torch.from_numpy(datastore_keys.transpose(1, 0))\n",
        "keys.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqNb_1I9vGjP",
        "outputId": "0f2ea3aa-cd1d-4c9c-9a39-1ecbc3ffc2a9"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([768, 390900])"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def choose(embeddings, label_ids, logits, inv_label_map):\n",
        "    batch_size, seq_len, _ = embeddings.shape\n",
        "    label_ids = label_ids.detach().cpu().numpy()\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    embeddings = embeddings.detach().cpu().numpy()\n",
        "\n",
        "    ignored_index = torch.nn.CrossEntropyLoss().ignore_index\n",
        "\n",
        "    valid_mask = label_ids != ignored_index\n",
        "\n",
        "    # Use boolean indexing to filter out the embeddings and logits\n",
        "    filtered_embeddings = embeddings[valid_mask]\n",
        "    filtered_logits = logits[valid_mask]\n",
        "    filtered_labels = label_ids[valid_mask]\n",
        "\n",
        "    # Map labels using inv_label_map, vectorized operation\n",
        "    out_label_list = [inv_label_map[label.item()] for label in filtered_labels]\n",
        "\n",
        "    assert len(filtered_embeddings) == len(filtered_logits)\n",
        "    assert len(filtered_embeddings) == len(filtered_labels)\n",
        "    return torch.tensor(filtered_embeddings), out_label_list, torch.tensor(filtered_logits)"
      ],
      "metadata": {
        "id": "MW-3fuuww-XX"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess_logits_to_labels(logits, embedding_test, keys, datastore_values_mapped):\n",
        "  #i = 2\n",
        "  batch = embedding_test.shape[0]\n",
        "  num_labels=42\n",
        "  hidden_size = embedding_test.shape[-1]\n",
        "  ##########\n",
        "  embedding_test = embedding_test.view(-1, hidden_size)\n",
        "  ##########\n",
        "  #print(embedding_test.shape)\n",
        "\n",
        "  sim = torch.mm(embedding_test, keys)\n",
        "\n",
        "  #print(sim.shape)\n",
        "\n",
        "  #norm_1 = (keys ** 2).sum(dim=0, keepdim=True).sqrt()\n",
        "  #norm_2 = (embedding_test ** 2).sum(dim=1, keepdim=True).sqrt()\n",
        "\n",
        "  norm_keys = torch.norm(keys, dim=0, keepdim=True)\n",
        "  norm_embeddings = torch.norm(embedding_test, dim=1, keepdim=True)\n",
        "\n",
        "\n",
        "\n",
        "  #scores = (sim / (norm_1 + 1e-10) / (norm_2 + 1e-10)).view(1, embedding_test.shape[0], -1)\n",
        "  scores = sim / (norm_keys * norm_embeddings + 1e-10)\n",
        "  scores = scores.view(1, embedding_test.shape[0], -1)\n",
        "\n",
        "  #print(scores.shape)\n",
        "\n",
        "  topk_scores, topk_idxs = torch.topk(scores, dim=-1, k=512)\n",
        "  #print(topk_idxs[:,i])\n",
        "  #print(topk_scores[:,i])\n",
        "\n",
        "  datastore_size = keys.shape[1]\n",
        "  #print(datastore_size)\n",
        "  knn_labels = datastore_values_mapped.unsqueeze(0)\n",
        "  #print(knn_labels)\n",
        "  knn_labels = knn_labels.view(1, 1, datastore_size).expand(1, batch, datastore_size)\n",
        "  knn_labels = knn_labels.gather(dim=-1, index=topk_idxs)\n",
        "  #print(knn_labels[:,i])\n",
        "\n",
        "  sim_probs = torch.softmax(topk_scores/0.5 , dim=-1)\n",
        "  #print(sim_probs[:,i])\n",
        "  knn_probabilities = torch.zeros_like(sim_probs[:, :, 0]).unsqueeze(-1).repeat([1,1,num_labels])\n",
        "  knn_probabilities = knn_probabilities.scatter_add(dim=2, index=knn_labels, src=sim_probs)\n",
        "  knn_probabilities = knn_probabilities.squeeze(0)\n",
        "\n",
        "  return knn_probabilities, logits"
      ],
      "metadata": {
        "id": "7v78R4xctKL-"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def knn_inference(model, device, loader, keys, values_mapped):\n",
        "    text_tokens = []\n",
        "    all_data_logits = []\n",
        "    all_knn_logits = []\n",
        "    all_labels = []\n",
        "\n",
        "    model.eval()\n",
        "    for batch in tqdm(loader):\n",
        "        batch = {k: v.to(device) if 'texts' not in k else v for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            input_ids = batch['input_ids']\n",
        "            attention_mask = batch['attention_mask']\n",
        "            token_type_ids = batch['token_type_ids']\n",
        "            labels = batch['label']\n",
        "            outputs = model(input_ids, attention_mask, token_type_ids,output_hidden_states=True)\n",
        "\n",
        "            test_embeddings = outputs.hidden_states[-1]\n",
        "            test_logits = outputs.logits\n",
        "            test_labels = labels\n",
        "\n",
        "            aligned_embeddings, aligned_labels, aligned_logits = choose(test_embeddings, test_labels, test_logits, inv_label_map)\n",
        "\n",
        "            knn_logits, logits = postprocess_logits_to_labels(aligned_logits, aligned_embeddings, keys, datastore_values_mapped)\n",
        "\n",
        "            text_tokens.append(batch['texts'])\n",
        "            all_data_logits.append(logits)\n",
        "            all_knn_logits.append(knn_logits)\n",
        "            all_labels.append(aligned_labels)\n",
        "    return text_tokens, all_data_logits, all_knn_logits, all_labels"
      ],
      "metadata": {
        "id": "Yt0e90AXtKPF"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_tokens, all_data_logits, all_knn_logits, all_labels = knn_inference(model_loaded, device, test_loader, keys, datastore_values_mapped)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9NFaJV1IXWI",
        "outputId": "255df73f-eb78-4117-c1c8-4cb817533d7c"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/207 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "100%|██████████| 207/207 [04:21<00:00,  1.26s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_tokens), len(all_data_logits), len(all_knn_logits), len(all_labels)"
      ],
      "metadata": {
        "id": "pbx3evVJtKR_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da5ed12b-9894-4e06-faa3-23cedb0e1732"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(207, 207, 207, 207)"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lambda_ = 0.0\n",
        "\n",
        "for lambda_ in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
        "  predicted_tag = []\n",
        "  true_tag = []\n",
        "  for i in range(len(all_data_logits)):\n",
        "    data_logtis = all_data_logits[i].squeeze(0)\n",
        "    knn_logits = all_knn_logits[i].squeeze(0)\n",
        "\n",
        "    probabilities = lambda_*data_logtis + (1-lambda_)*knn_logits\n",
        "\n",
        "    argmax_labels = torch.argmax(probabilities, -1, keepdim=False)\n",
        "    argmax_labels = argmax_labels.detach().cpu().numpy()\n",
        "\n",
        "    predicted_tag.append([inv_label_map[label] for label in argmax_labels])\n",
        "    true_tag.append(all_labels[i])\n",
        "\n",
        "  print('f1:        ',f1_score(true_tag, predicted_tag))\n",
        "  #print('acc:       ', accuracy_score(true_tag, predicted_tag))\n",
        "  #print('precision: ', precision_score(true_tag, predicted_tag))\n",
        "  #print('recall:    ', recall_score(true_tag, predicted_tag))\n",
        "  print('----------------------------------------')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZ0IsmBd1rVZ",
        "outputId": "77a69c1f-8f3d-47e7-a24c-5d252ae8aa84"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1:         0.9022711631108054\n",
            "----------------------------------------\n",
            "f1:         0.9054993461353155\n",
            "----------------------------------------\n",
            "f1:         0.9060583086360191\n",
            "----------------------------------------\n",
            "f1:         0.9066960899248327\n",
            "----------------------------------------\n",
            "f1:         0.9067825846960601\n",
            "----------------------------------------\n",
            "f1:         0.9077475486811214\n",
            "----------------------------------------\n",
            "f1:         0.9082619508151423\n",
            "----------------------------------------\n",
            "f1:         0.9080610623748013\n",
            "----------------------------------------\n",
            "f1:         0.9083120292959304\n",
            "----------------------------------------\n",
            "f1:         0.9080102287649459\n",
            "----------------------------------------\n",
            "f1:         0.9076582803428257\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5WOnFKRJss4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S4tPA5pYss62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xafr8EsSss87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i55tvWEwss_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a1foIwfMstDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-F3p7iWXfA3d"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}