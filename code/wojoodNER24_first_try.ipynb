{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "1ZD31r2uf-Ux"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4TETlBEcuG3",
        "outputId": "e96e9dd7-e118-414a-ff5e-38391af62d2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla V100-SXM2-16GB\n",
            "Wed Apr  3 04:39:04 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla V100-SXM2-16GB           Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0              26W / 300W |      2MiB / 16384MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "    !nvidia-smi\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.12.2\n",
        "!pip install farasapy==0.0.14\n",
        "!pip install pyarabic==0.6.14\n",
        "!git clone https://github.com/aub-mind/arabert\n",
        "!pip install emoji==1.6.1\n",
        "!pip install sentencepiece==0.1.96\n",
        "!pip install seqeval\n",
        "!pip install accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_YuRDWzcv8g",
        "outputId": "0dedb643-36bc-400b-ee3f-f2f9ca5b0eae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.12.2\n",
            "  Using cached transformers-4.12.2-py3-none-any.whl (3.1 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.12.2) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.12.2) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.12.2) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.12.2) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.12.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.12.2) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.12.2) (2.31.0)\n",
            "Collecting sacremoses (from transformers==4.12.2)\n",
            "  Using cached sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "Collecting tokenizers<0.11,>=0.10.1 (from transformers==4.12.2)\n",
            "  Using cached tokenizers-0.10.3.tar.gz (212 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.12.2) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.17->transformers==4.12.2) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.17->transformers==4.12.2) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.12.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.12.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.12.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.12.2) (2024.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.12.2) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.12.2) (1.3.2)\n",
            "Building wheels for collected packages: tokenizers\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build tokenizers\n",
            "\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: farasapy==0.0.14 in /usr/local/lib/python3.10/dist-packages (0.0.14)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from farasapy==0.0.14) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from farasapy==0.0.14) (4.66.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy==0.0.14) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy==0.0.14) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy==0.0.14) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy==0.0.14) (2024.2.2)\n",
            "Requirement already satisfied: pyarabic==0.6.14 in /usr/local/lib/python3.10/dist-packages (0.6.14)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from pyarabic==0.6.14) (1.16.0)\n",
            "fatal: destination path 'arabert' already exists and is not an empty directory.\n",
            "Requirement already satisfied: emoji==1.6.1 in /usr/local/lib/python3.10/dist-packages (1.6.1)\n",
            "Requirement already satisfied: sentencepiece==0.1.96 in /usr/local/lib/python3.10/dist-packages (0.1.96)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.4.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.99)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AhmedAbdel-Aal/WNER_24_sharedtask.git\n",
        "!cp -r WNER_24_sharedtask/code/*.py .\n",
        "!rm -rf WNER_24_sharedtask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djzOxAz4x7sV",
        "outputId": "631ad66e-c4b0-40ae-a192-d78c1f6d03f0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'WNER_24_sharedtask'...\n",
            "remote: Enumerating objects: 51, done.\u001b[K\n",
            "remote: Counting objects: 100% (51/51), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 51 (delta 18), reused 48 (delta 15), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (51/51), 10.45 KiB | 10.45 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import copy\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from arabert.preprocess import ArabertPreprocessor\n",
        "from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import (AutoConfig, AutoModel, AutoModelForSequenceClassification,\n",
        "                          AutoTokenizer, BertTokenizer, Trainer,\n",
        "                          TrainingArguments)\n",
        "from transformers.data.processors.utils import InputFeatures\n",
        "\n",
        "from arabert.preprocess import ArabertPreprocessor\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoConfig, AutoModelForTokenClassification, AutoTokenizer\n",
        "from transformers import Trainer , TrainingArguments\n",
        "from transformers.trainer_utils import EvaluationStrategy\n",
        "from transformers.data.processors.utils import InputFeatures\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.utils import resample\n",
        "import logging\n",
        "import torch\n",
        "from utils import *"
      ],
      "metadata": {
        "id": "Z89S1TxOcv_M"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = load_json('/content/drive/MyDrive/anlp24/split70.json')\n",
        "dev_data = load_json('/content/drive/MyDrive/anlp24/split10.json')\n",
        "\n",
        "def normalize_data(items):\n",
        "    normalized_data = []\n",
        "    text_lists = []\n",
        "    label_list = []\n",
        "    for item in items:\n",
        "        sentence_id = item['global_sentence_id']\n",
        "        sentence = []\n",
        "        labels = []\n",
        "        for token_info in item['tokens']:\n",
        "            word = token_info['token']\n",
        "            tv = token_info['tags'][0]\n",
        "            label = tv['value']\n",
        "            sentence.append(word)\n",
        "            labels.append(label)\n",
        "        #sentence = ' '.join(sentence)\n",
        "        #labels = ' '.join(labels)\n",
        "        text_lists.append(sentence)\n",
        "        label_list.append(labels)\n",
        "    return text_lists, label_list\n",
        "\n",
        "\n",
        "# Normalize the data\n",
        "text_train, labels_train = normalize_data(train_data)\n",
        "text_dev, labels_dev = normalize_data(dev_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uij3wvOKcwBt",
        "outputId": "d0abaa32-7a58-420b-9d03-49d84a8caba6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data loaded from path /content/drive/MyDrive/anlp24/split70.json\n",
            "data loaded from path /content/drive/MyDrive/anlp24/split10.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_labels = list(set([label for sublist in labels_train for label in sublist]))\n",
        "label_map = { v:index for index, v in enumerate(all_labels)}\n",
        "inv_label_map = {i: label for i, label in enumerate(all_labels)}"
      ],
      "metadata": {
        "id": "z2ISKV9kyg7u"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from data import NERDataset"
      ],
      "metadata": {
        "id": "9i6YXb3ZcwEV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'aubmindlab/bert-base-arabertv02'\n",
        "task_name = 'tokenclassification'"
      ],
      "metadata": {
        "id": "jyuCGIq9cwG5"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_path = '/content/drive/MyDrive/anlp24/nerflat-arabertv02-e3/output_dir'\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)"
      ],
      "metadata": {
        "id": "gp2J446ZRUnU"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = NERDataset(\n",
        "    texts=text_train,\n",
        "    tags=labels_train,\n",
        "    label_list=all_labels,\n",
        "    model_name=model_name,\n",
        "    max_length=512,\n",
        "    tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "dev_dataset = NERDataset(\n",
        "    texts=text_dev,\n",
        "    tags=labels_dev,\n",
        "    label_list=all_labels,\n",
        "    model_name=model_name,\n",
        "    max_length=512,\n",
        "    tokenizer=tokenizer\n",
        "    )"
      ],
      "metadata": {
        "id": "_pIJRVA0cwLV"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_init():\n",
        "    return AutoModelForTokenClassification.from_pretrained(model_name, return_dict=True, num_labels=len(label_map))"
      ],
      "metadata": {
        "id": "-ZMg1-WHdk0Z"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_init_trained():\n",
        "    return AutoModelForTokenClassification.from_pretrained(\"/content/drive/MyDrive/anlp24/nerflat-arabertv02-e3/output_dir\").to(device)"
      ],
      "metadata": {
        "id": "qEy0rQUlHzpa"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def align_predictions(predictions, label_ids):\n",
        "    preds = np.argmax(predictions, axis=2)\n",
        "\n",
        "    batch_size, seq_len = preds.shape\n",
        "\n",
        "    out_label_list = [[] for _ in range(batch_size)]\n",
        "    preds_list = [[] for _ in range(batch_size)]\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        for j in range(seq_len):\n",
        "            if label_ids[i, j] != torch.nn.CrossEntropyLoss().ignore_index:\n",
        "                out_label_list[i].append(inv_label_map[label_ids[i][j]])\n",
        "                preds_list[i].append(inv_label_map[preds[i][j]])\n",
        "\n",
        "    return preds_list, out_label_list\n",
        "\n",
        "def compute_metrics(p):\n",
        "    preds_list, out_label_list = align_predictions(p.predictions,p.label_ids)\n",
        "    return {\n",
        "        \"accuracy_score\": accuracy_score(out_label_list, preds_list),\n",
        "        \"precision\": precision_score(out_label_list, preds_list),\n",
        "        \"recall\": recall_score(out_label_list, preds_list),\n",
        "        \"f1\": f1_score(out_label_list, preds_list),\n",
        "    }"
      ],
      "metadata": {
        "id": "-8Cn1GYpdk28"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "D6n-7LlJ_fka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir= \"./train\",\n",
        "    adam_epsilon = 1e-8,\n",
        "    learning_rate = 3e-5,\n",
        "    fp16 = True, # enable this when using V100 or T4 GPU\n",
        "    per_device_train_batch_size = 8, # up to 64 on 16GB with max len of 128\n",
        "    per_device_eval_batch_size = 8,\n",
        "    gradient_accumulation_steps = 1, # use this to scale batch size without needing more memory\n",
        "    num_train_epochs= 2,\n",
        "    warmup_ratio = 0,\n",
        "    do_eval = True,\n",
        "    evaluation_strategy = 'epoch',\n",
        "    save_strategy = 'epoch',\n",
        "    load_best_model_at_end = True, # this allows to automatically get the best model at the end based on whatever metric we want\n",
        "    metric_for_best_model = 'f1',\n",
        "    greater_is_better = True,\n",
        "    seed = 25\n",
        "  )\n",
        "def set_seed(seed=42):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  torch.backends.cudnn.deterministic=True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(25)"
      ],
      "metadata": {
        "id": "_BRkqsaUdk5V"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=dev_dataset,\n",
        "    model_init=model_init_trained,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "Z0d--d9gdk7d"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "O7W5gP7SdqWN",
        "outputId": "6de0d0f5-684c-4479-b128-487525dae547"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5782' max='5782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5782/5782 13:24, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy Score</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.112300</td>\n",
              "      <td>0.096251</td>\n",
              "      <td>0.969868</td>\n",
              "      <td>0.882217</td>\n",
              "      <td>0.908633</td>\n",
              "      <td>0.895230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.065400</td>\n",
              "      <td>0.095438</td>\n",
              "      <td>0.972944</td>\n",
              "      <td>0.896764</td>\n",
              "      <td>0.918847</td>\n",
              "      <td>0.907671</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=5782, training_loss=0.11870216078017666, metrics={'train_runtime': 804.6367, 'train_samples_per_second': 57.479, 'train_steps_per_second': 7.186, 'total_flos': 1.208934537984e+16, 'train_loss': 0.11870216078017666, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "0FV27LDE35Ee",
        "outputId": "e5d4e282-7b65-4fb9-ed7a-d61ec9d28396"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='413' max='413' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [413/413 00:18]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 14.253826141357422,\n",
              " 'eval_accuracy_score': 0.02716040801431873,\n",
              " 'eval_precision': 0.00034859400418312805,\n",
              " 'eval_recall': 0.002518539247236603,\n",
              " 'eval_f1': 0.0006124219587295646,\n",
              " 'eval_runtime': 43.7544,\n",
              " 'eval_samples_per_second': 75.512,\n",
              " 'eval_steps_per_second': 9.439}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "chfORcv-QTw9",
        "outputId": "2db9bc65-5af9-4b40-cccc-08cffeb5c54f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='413' max='413' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [413/413 00:19]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 14.253826141357422,\n",
              " 'eval_accuracy_score': 0.02716040801431873,\n",
              " 'eval_precision': 0.00034859400418312805,\n",
              " 'eval_recall': 0.002518539247236603,\n",
              " 'eval_f1': 0.0006124219587295646,\n",
              " 'eval_runtime': 45.0139,\n",
              " 'eval_samples_per_second': 73.399,\n",
              " 'eval_steps_per_second': 9.175}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.model.config.label2id = label_map\n",
        "trainer.model.config.id2label = inv_label_map\n",
        "trainer.save_model(\"output_dir\")\n",
        "train_dataset.tokenizer.save_pretrained(\"output_dir\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VMQENTDSs3D",
        "outputId": "def03458-9f03-4f3b-df12-f7706b820f83"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('output_dir/tokenizer_config.json',\n",
              " 'output_dir/special_tokens_map.json',\n",
              " 'output_dir/vocab.txt',\n",
              " 'output_dir/added_tokens.json',\n",
              " 'output_dir/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/output_dir /content/drive/MyDrive/anlp24/nerflat-arabertv02-e3"
      ],
      "metadata": {
        "id": "Zpz_fEjCTbe9"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# load model"
      ],
      "metadata": {
        "id": "EQpGxGpGzWSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: load the saved model\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/anlp24/nerflat-arabertv02-e3/output_dir\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"/content/drive/MyDrive/anlp24/nerflat-arabertv02-e3/output_dir\").to(device)\n"
      ],
      "metadata": {
        "id": "MNhtv4vJdqbd"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate_fn function to handle batching of InputFeatures.\n",
        "\n",
        "    Args:\n",
        "    - batch: A list of InputFeatures instances.\n",
        "\n",
        "    Returns:\n",
        "    - A dictionary with keys 'input_ids', 'attention_mask', 'token_type_ids', 'labels',\n",
        "      where the values are batched tensors.\n",
        "    \"\"\"\n",
        "    # Initialize containers for the various features\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    token_type_ids = []\n",
        "    labels = []\n",
        "\n",
        "\n",
        "    for item in batch:\n",
        "        #print(item)\n",
        "        input_ids.append(torch.tensor(item['input_ids']))  # Convert to tensor\n",
        "        attention_masks.append(torch.tensor(item['attention_mask']))  # Convert to tensor\n",
        "        token_type_ids.append(torch.tensor(item['token_type_ids']))  # Convert to tensor\n",
        "        labels.append(torch.tensor(item['labels']))\n",
        "\n",
        "    # Convert lists to tensors and stack them\n",
        "    input_ids = torch.stack(input_ids)\n",
        "    attention_masks = torch.stack(attention_masks)\n",
        "    token_type_ids = torch.stack(token_type_ids)\n",
        "    labels = torch.stack(labels)\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_masks,\n",
        "        'token_type_ids': token_type_ids,\n",
        "        'label': labels\n",
        "    }\n",
        "\n",
        "# Prepare DataLoader'\n",
        "batch_size = 16  # Or any batch size that fits your memory\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False, num_workers=5)\n",
        "test_loader = DataLoader(dev_dataset, batch_size=batch_size,collate_fn=collate_fn, shuffle=False, num_workers=5)"
      ],
      "metadata": {
        "id": "CA7mUXDg4dMd"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN"
      ],
      "metadata": {
        "id": "qsWRl0pZ6jVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _normalizer(x, epsilon=1e-10) -> np.ndarray:\n",
        "        return x / (np.linalg.norm(x, axis=-1, keepdims=True) + epsilon)"
      ],
      "metadata": {
        "id": "zgwi_is76nqU"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_index(embeddings, label_ids):\n",
        "    batch_size, seq_len, _ = embeddings.shape\n",
        "    label_ids = label_ids.detach().cpu().numpy()\n",
        "    embeddings = embeddings.detach().cpu().numpy()\n",
        "\n",
        "    out_label_list = []\n",
        "    in_embeddings = []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        for j in range(seq_len):\n",
        "            if label_ids[i, j] != torch.nn.CrossEntropyLoss().ignore_index:\n",
        "                out_label_list.append(inv_label_map[label_ids[i][j]])\n",
        "                in_embeddings.append(embeddings[i][j])\n",
        "    return in_embeddings, out_label_list\n"
      ],
      "metadata": {
        "id": "SqDUbQvq6E30"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "tSmsHfJX-S82"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_embeddings = []\n",
        "train_labels = []\n",
        "\n",
        "model.eval()\n",
        "for batch in tqdm(train_loader):\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        token_type_ids = batch['token_type_ids']\n",
        "        labels = batch['label']\n",
        "        outputs = model(input_ids, attention_mask, token_type_ids,output_hidden_states=True)\n",
        "        embeddings = outputs.hidden_states[-1]\n",
        "\n",
        "        in_embeddings, in_labels = create_index(embeddings, labels)\n",
        "        train_embeddings.append(in_embeddings)\n",
        "        train_labels.append(in_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C36aDrIz4dSc",
        "outputId": "70d54ffa-c633-4f2b-ee6a-86ad1264ce4f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1446/1446 [07:00<00:00,  3.44it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_embeddings), len(train_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1G6LNElEBKiN",
        "outputId": "eda4aacd-6361-4f8b-8d87-ddc90f4edbbf"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1446, 1446)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datastore_keys = [item for sublist in train_embeddings for item in sublist]\n",
        "datastore_labels = [item for sublist in train_labels for item in sublist]"
      ],
      "metadata": {
        "id": "kPZYcN2tBocm"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(datastore_keys), len(datastore_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQl-aO-IBofG",
        "outputId": "57854e6f-2cda-4e4e-eb10-90af7eaad184"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(390900, 390900)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1446*16*512"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABTxsp3HD4Z5",
        "outputId": "994df4a7-a95c-4bc2-d402-3322a7e1e243"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11845632"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datastore_keys = np.array(datastore_keys)\n",
        "datastore_labels = np.array(datastore_labels)"
      ],
      "metadata": {
        "id": "eaA9-nRPFFV8"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datastore_keys.shape, datastore_labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovIvbk9nFFiy",
        "outputId": "10f115e5-7d63-4c1b-c00a-36a61347df42"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((390900, 768), (390900,))"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datastore_values = [label_map[label] for label in datastore_labels]\n",
        "datastore_values = np.array(datastore_values)"
      ],
      "metadata": {
        "id": "JoiIewh5H5Hi"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save the numpy arrays datastore_keys and datastore_values\n",
        "\n",
        "import numpy as np\n",
        "np.save('/content/drive/MyDrive/anlp24/nerflat-arabertv02-e3/output_dir/datastore_keys.npy', datastore_keys)\n",
        "np.save('/content/drive/MyDrive/anlp24/nerflat-arabertv02-e3/output_dir/datastore_values.npy', datastore_values)\n"
      ],
      "metadata": {
        "id": "glVP-_6AAQ1Z"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def choose(embeddings, label_ids, logits):\n",
        "    batch_size, seq_len, _ = embeddings.shape\n",
        "    label_ids = label_ids.detach().cpu().numpy()\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    embeddings = _normalizer(embeddings.detach().cpu().numpy())\n",
        "\n",
        "    out_label_list = []\n",
        "    in_embeddings = []\n",
        "    in_logits = []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        for j in range(seq_len):\n",
        "            if label_ids[i, j] != torch.nn.CrossEntropyLoss().ignore_index:\n",
        "                out_label_list.append(inv_label_map[label_ids[i][j]])\n",
        "                in_embeddings.append(embeddings[i][j])\n",
        "                in_logits.append(logits[i][j])\n",
        "    return in_embeddings, out_label_list, in_logits\n"
      ],
      "metadata": {
        "id": "va8SnMp8GZkw"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_embeddings = []\n",
        "test_labels = []\n",
        "test_logits = []\n",
        "\n",
        "model.eval()\n",
        "for batch in tqdm(test_loader):\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        token_type_ids = batch['token_type_ids']\n",
        "        labels = batch['label']\n",
        "        outputs = model(input_ids, attention_mask, token_type_ids,output_hidden_states=True)\n",
        "        embeddings = outputs.hidden_states[-1]\n",
        "        logits = outputs.logits\n",
        "        in_embeddings, in_labels, in_logits = choose(embeddings, labels, logits)\n",
        "        test_embeddings.append(in_embeddings)\n",
        "        test_labels.append(in_labels)\n",
        "        test_logits.append(in_logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8uY3W4SD_Q-",
        "outputId": "790a302b-d244-452c-ceab-a19c4fc793fc"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 207/207 [01:05<00:00,  3.14it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_logits[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuONCkGcGDOr",
        "outputId": "04317b28-a938-4222-a1fb-ec7f4b5d81bc"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_embeddings = [item for sublist in test_embeddings for item in sublist]\n",
        "test_labels = [item for sublist in test_labels for item in sublist]\n",
        "test_logits = [item for sublist in test_logits for item in sublist]"
      ],
      "metadata": {
        "id": "SQchVE-zD_T0"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_embeddings = np.array(test_embeddings)\n",
        "test_labels = np.array(test_labels)\n",
        "test_logits = np.array(test_logits)"
      ],
      "metadata": {
        "id": "cIFlrjpcD_Y3"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_values = [label_map[label] for label in test_labels]\n",
        "test_values = np.array(test_values)"
      ],
      "metadata": {
        "id": "2zRevAIFIPXF"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_embeddings.shape, test_values.shape, test_logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_PQ0xFHD_bX",
        "outputId": "80cc7dff-4dc1-41f8-d894-f81b30643d07"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((57547, 768), (57547,), (57547, 42))"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('/content/drive/MyDrive/anlp24/nerflat-arabertv02-e3/test_embeddings.npy', test_embeddings)\n",
        "np.save('/content/drive/MyDrive/anlp24/nerflat-arabertv02-e3/test_labels.npy', test_values)\n",
        "np.save('/content/drive/MyDrive/anlp24/nerflat-arabertv02-e3/test_logits.npy', test_logits)\n"
      ],
      "metadata": {
        "id": "jIhTc0UhAkfN"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hm84__vpD_ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r03MEVn44o4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S9ROreae4o7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aDS7sQge4o_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JFbiiDZ6dqgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN"
      ],
      "metadata": {
        "id": "onvIHCOizbww"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bEEtByNPdqi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iboyC_fjzbgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zoPKqkjnzbjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2DxwyuPEzbl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V5us6P3Xzboj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conformal Prediction"
      ],
      "metadata": {
        "id": "woEByQu8zYSq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YMP3wEn7dqln"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}